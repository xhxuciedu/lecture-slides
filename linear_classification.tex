% Reset frame number to 1
\setcounter{framenumber}{0}

% Section title slide
\begin{frame}
\frametitle{Linear Classification}
\begin{center}
\Large \textbf{Linear Classification}
\end{center}
\end{frame}

\section{Linear Classification}

\begin{frame}{Motivation}
  \begin{itemize}
    \item Core tool for binary and multi-class prediction.
    \item Interpretable decision boundary and scalable training.
    \item Foundation for logistic regression and neural nets.
  \end{itemize}
\end{frame}

\begin{frame}{Learning Objectives}
  \begin{itemize}
    \item Define linear classifiers and hypothesis classes.
    \item Understand why 0--1 loss is hard to optimize.
    \item Derive binary NLL (cross-entropy) from a probabilistic model.
    \item Compute gradients and apply gradient descent.
    \item Extend to multi-class with softmax and one-hot targets.
    \item Implement gradient descent in Python.
  \end{itemize}
\end{frame}

\begin{frame}{Supervised Classification Setup}
  \begin{block}{Data}
    \[
      \mathcal{D} = \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^m, \quad \mathbf{x}^{(i)} \in \R^n, \; y^{(i)} \in \{0,1\}
    \]
  \end{block}
  \begin{itemize}
    \item Goal: learn a classifier that predicts labels for new inputs.
    \item We focus on linear decision boundaries.
  \end{itemize}
\end{frame}

\begin{frame}{Hypothesis Class: Linear Classifiers}
  \[
    f_{\boldsymbol{w},b}(\mathbf{x}) = \boldsymbol{w}^T \mathbf{x} + b
  \]
  Prediction rule:
  \[
    \hat{y} = \mathbf{1}[f_{\boldsymbol{w},b}(\mathbf{x}) \ge 0]
  \]
  \begin{itemize}
    \item \(\boldsymbol{w} \in \R^n\), \(b \in \R\).
    \item Decision boundary: \( \boldsymbol{w}^T \mathbf{x} + b = 0 \).
  \end{itemize}
\end{frame}

\begin{frame}{Geometric Interpretation}
  \begin{itemize}
    \item \(\boldsymbol{w}\) is normal to the separating hyperplane.
    \item Signed distance (up to scale):
      \[
        \text{margin} = \frac{\boldsymbol{w}^T \mathbf{x} + b}{\|\boldsymbol{w}\|_2}
      \]
    \item Larger margin \(\Rightarrow\) more confident predictions.
  \end{itemize}
\end{frame}

\begin{frame}{Linear Separability}
  \begin{itemize}
    \item If classes are linearly separable, some \((\boldsymbol{w}, b)\) achieves zero training error.
    \item If not separable, we fit a probabilistic model and minimize NLL.
  \end{itemize}
\end{frame}

\begin{frame}{0--1 Loss}
  \[
    \ell_{0/1}(y, \hat{y}) = \mathbf{1}[y \ne \hat{y}]
  \]
  \begin{itemize}
    \item Directly measures classification error.
    \item Nonconvex and non-differentiable \(\Rightarrow\) hard to optimize.
  \end{itemize}
\end{frame}

\begin{frame}{Why 0--1 Loss Is Difficult}
  \begin{itemize}
    \item Discontinuous objective with many flat regions.
    \item Gradient-based methods do not apply.
    \item Leads to combinatorial optimization.
  \end{itemize}
  \begin{block}{Idea}
    Replace with a convex, differentiable surrogate.
  \end{block}
\end{frame}

\begin{frame}{Probabilistic Model (Binary)}
  Define a score:
  \[
    s(\mathbf{x}) = \boldsymbol{w}^T \mathbf{x} + b
  \]
  Logistic model:
  \[
    P(y=1 \mid \mathbf{x}) = \sigma(s(\mathbf{x})), \quad \sigma(t) = \frac{1}{1+e^{-t}}
  \]
  \[
    P(y \mid \mathbf{x}) = \text{Bernoulli}(y; \sigma(s(\mathbf{x})))
  \]
\end{frame}

\begin{frame}{Binary NLL (Single Example)}
  For \(y \in \{0,1\}\) and \(p = \sigma(s(\mathbf{x}))\):
  \[
    \ell(\boldsymbol{w}, b; \mathbf{x}, y)
    = -\log P(y \mid \mathbf{x})
    = -\left[y \log p + (1-y)\log(1-p)\right]
  \]
  \begin{itemize}
    \item Negative log-likelihood (NLL) of a Bernoulli model.
    \item Also called binary cross-entropy loss.
  \end{itemize}
\end{frame}

\begin{frame}{Binary Cross-Entropy (Dataset)}
  \[
    \mathcal{L}(\boldsymbol{w}, b) =
    \frac{1}{m}\sum_{i=1}^m
    \left[-y^{(i)} \log p^{(i)} - (1-y^{(i)})\log(1-p^{(i)})\right]
  \]
  where \(p^{(i)} = \sigma(\boldsymbol{w}^T \mathbf{x}^{(i)} + b)\).
  \begin{itemize}
    \item Convex in \((\boldsymbol{w}, b)\).
    \item Enables global optimum with gradient descent.
  \end{itemize}
\end{frame}

\begin{frame}{Convexity and Implications}
  \begin{itemize}
    \item Binary NLL for logistic regression is convex.
    \item Any local minimum is a global minimum.
    \item Gradient descent converges with an appropriate step size.
  \end{itemize}
\end{frame}

\begin{frame}{Regularization}
  \[
    \min_{\boldsymbol{w}, b} \; \mathcal{L}(\boldsymbol{w}, b) + \frac{\lambda}{2} \|\boldsymbol{w}\|_2^2
  \]
  \begin{itemize}
    \item Controls complexity and improves generalization.
    \item Encourages smaller weights and more stable decision boundaries.
    \item Multi-class: add \(\frac{\lambda}{2}\|\Theta\|_F^2\).
  \end{itemize}
\end{frame}

\begin{frame}{Gradient of Binary NLL}
  Let \(\mathbf{p} = \sigma(X\boldsymbol{w} + b\mathbf{1}) \in \R^m\).
  \[
    \nabla_{\boldsymbol{w}} \mathcal{L} = \frac{1}{m} X^T (\mathbf{p} - \mathbf{y})
  \]
  \[
    \frac{\partial \mathcal{L}}{\partial b} = \frac{1}{m} \mathbf{1}^T (\mathbf{p} - \mathbf{y})
  \]
  \begin{itemize}
    \item Gradient is proportional to prediction error \((\mathbf{p} - \mathbf{y})\).
    \item With L2 regularization: add \(\lambda \boldsymbol{w}\) to \(\nabla_{\boldsymbol{w}}\).
  \end{itemize}
\end{frame}

\begin{frame}{Gradient Descent Update}
  \[
    \boldsymbol{w} \leftarrow \boldsymbol{w} - \eta \nabla_{\boldsymbol{w}} \mathcal{L},
    \quad
    b \leftarrow b - \eta \frac{\partial \mathcal{L}}{\partial b}
  \]
  \begin{itemize}
    \item \(\eta\): learning rate.
    \item Use batch, mini-batch, or stochastic gradients.
  \end{itemize}
\end{frame}

\begin{frame}{Gradient Descent Pseudocode (Binary)}
  \begin{algorithm}[H]
  \SetAlgoLined
  \KwIn{Data $X$, labels $\mathbf{y} \in \{0,1\}^m$, step size $\eta$, steps $T$}
  \KwOut{Parameters $\boldsymbol{w}, b$}
  Initialize $\boldsymbol{w} \leftarrow 0$, $b \leftarrow 0$\;
  \For{$t=1$ \textbf{to} $T$}{
    $\mathbf{p} \leftarrow \sigma(X\boldsymbol{w} + b\mathbf{1})$\;
    $\boldsymbol{w} \leftarrow \boldsymbol{w} - \eta \frac{1}{m} X^T (\mathbf{p}-\mathbf{y})$\;
    $b \leftarrow b - \eta \frac{1}{m} \mathbf{1}^T (\mathbf{p}-\mathbf{y})$\;
  }
  \end{algorithm}
\end{frame}


\begin{frame}{Variants of Gradient Descent}
  \begin{itemize}
    \item \textbf{Batch GD}: full dataset per update.
    \item \textbf{SGD}: one example per update.
    \item \textbf{Mini-batch}: small batch per update.
  \end{itemize}
  \begin{itemize}
    \item Trade-offs: stability vs.\ speed vs.\ variance.
  \end{itemize}
\end{frame}


\begin{frame}[fragile]{Logistic Regression in Python}
\begin{verbatim}
import numpy as np

def logistic_gd(X, y, eta=0.1, steps=1000):
    m, n = X.shape
    w = np.zeros(n)
    b = 0.0
    for _ in range(steps):
        scores = X @ w + b
        p = 1.0 / (1.0 + np.exp(-scores))
        grad_w = (1/m) * (X.T @ (p - y))
        grad_b = (1/m) * np.sum(p - y)
        w -= eta * grad_w
        b -= eta * grad_b
    return w, b
\end{verbatim}
\end{frame}

\begin{frame}{Prediction and Probabilities}
  Logistic regression outputs:
  \[
    P(y=1 \mid \mathbf{x}) = \sigma(\boldsymbol{w}^T \mathbf{x} + b)
  \]
  where \(\sigma(t) = 1/(1+e^{-t})\).
  \begin{itemize}
    \item Threshold at 0.5 for class prediction.
    \item Scores can be calibrated for decision-making.
  \end{itemize}
\end{frame}

\begin{frame}{Multi-class Classification}
  \begin{itemize}
    \item Classes: \(y \in \{1,\ldots,K\}\).
    \item One-vs-rest (OvR): train \(K\) binary classifiers.
    \item Multinomial (softmax) classifier: a single model for all classes.
  \end{itemize}
\end{frame}

\begin{frame}{Softmax Model and Parameters}
  Score for class \(k\):
  \[
    s_k(\mathbf{x}) = \boldsymbol{\theta}_k^T \mathbf{x}
  \]
  Softmax probabilities:
  \[
    P(y=k \mid \mathbf{x}) =
    \frac{e^{s_k(\mathbf{x})}}{\sum_{j=1}^K e^{s_j(\mathbf{x})}}
  \]
  \begin{itemize}
    \item Parameter vectors: \(\boldsymbol{\theta}_k \in \R^n\).
    \item Parameter matrix: \(\Theta = [\boldsymbol{\theta}_1,\ldots,\boldsymbol{\theta}_K] \in \R^{n \times K}\).
    \item Bias terms can be absorbed by augmenting \(\mathbf{x}\) with \(x_0=1\).
  \end{itemize}
\end{frame}

\begin{frame}{Softmax Properties}
  \begin{itemize}
    \item Produces a valid probability distribution over classes.
    \item Invariant to adding a constant to all scores.
    \item Sharpness controlled by relative score differences.
  \end{itemize}
  \[
    \sum_{k=1}^K P(y=k \mid \mathbf{x}) = 1
  \]
\end{frame}

\begin{frame}{Softmax: Stable Computation}
  For a score vector \(\mathbf{s} \in \R^K\):
  \[
    \text{softmax}(\mathbf{s})_k =
    \frac{e^{s_k - \max_j s_j}}{\sum_{j=1}^K e^{s_j - \max_\ell s_\ell}}
  \]
  \begin{itemize}
    \item Subtracting \(\max_j s_j\) prevents overflow.
    \item The distribution is unchanged by constant shifts.
  \end{itemize}
\end{frame}

\begin{frame}{Negative Log-Likelihood (NLL)}
  For one example \((\mathbf{x}, \mathbf{y})\) with one-hot \(\mathbf{y} \in \{0,1\}^K\):
  \[
    \ell(\Theta; \mathbf{x}, \mathbf{y})
    = -\sum_{k=1}^K y_k \log P(y=k \mid \mathbf{x})
  \]
  Using softmax:
  \[
    \ell = -\log \frac{e^{\boldsymbol{\theta}_{y}^T \mathbf{x}}}{\sum_{j=1}^K e^{\boldsymbol{\theta}_j^T \mathbf{x}}}
  \]
  \begin{itemize}
    \item This is cross-entropy between \(\mathbf{y}\) and predicted probabilities.
  \end{itemize}
\end{frame}

\begin{frame}{Cross-Entropy Loss (Dataset)}
  Stack one-hot labels into \(Y \in \R^{m \times K}\).
  \[
    \mathcal{L}(\Theta) =
    -\frac{1}{m}\sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} \log P(y=k \mid \mathbf{x}^{(i)})
  \]
  \begin{itemize}
    \item Exactly the average NLL over the dataset.
    \item Convex in \(\Theta\) for softmax regression.
  \end{itemize}
\end{frame}

\begin{frame}{Gradient of Softmax Loss}
  Let \(P \in \R^{m \times K}\) be predicted probabilities.
  \[
    \nabla_{\boldsymbol{\theta}_k} \mathcal{L}
    = \frac{1}{m}\sum_{i=1}^m \left(P_k^{(i)} - y_k^{(i)}\right)\mathbf{x}^{(i)}
  \]
  Matrix form:
  \[
    \nabla_{\Theta} \mathcal{L} = \frac{1}{m} X^T (P - Y)
  \]
\end{frame}

\begin{frame}{Gradient Descent Update}
  \[
    \Theta \leftarrow \Theta - \eta \nabla_{\Theta} \mathcal{L}
  \]
  \begin{itemize}
    \item Same optimization routines as binary logistic regression.
    \item Mini-batch SGD scales to large datasets.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Softmax Regression in Python}
\begin{verbatim}
import numpy as np

def softmax(Z):
    Z = Z - Z.max(axis=1, keepdims=True)
    expZ = np.exp(Z)
    return expZ / expZ.sum(axis=1, keepdims=True)

def softmax_gd(X, Y, eta=0.1, steps=500):
    m, n = X.shape
    K = Y.shape[1]
    Theta = np.zeros((n, K))
    for _ in range(steps):
        P = softmax(X @ Theta)
        grad = (1/m) * X.T @ (P - Y)
        Theta -= eta * grad
    return Theta
\end{verbatim}
\end{frame}

\begin{frame}{Feature Engineering}
  \begin{itemize}
    \item Linear in parameters does not imply linear in inputs.
    \item Use feature maps \(\boldsymbol{\phi}(\mathbf{x})\) to capture nonlinearity.
  \end{itemize}
  \[
    \hat{y} = \mathbf{1}[\boldsymbol{w}^T \boldsymbol{\phi}(\mathbf{x}) + b \ge 0]
  \]
\end{frame}

\begin{frame}{Common Pitfalls}
  \begin{itemize}
    \item Not scaling features (slow or unstable optimization).
    \item Using 0--1 loss directly (non-optimizable).
    \item Ignoring class imbalance (bias in predictions).
  \end{itemize}
\end{frame}

\begin{frame}{Summary}
  \begin{itemize}
    \item Linear classifiers predict via a hyperplane.
    \item Binary classification uses NLL / cross-entropy from a Bernoulli model.
    \item Convex losses enable reliable gradient descent.
    \item Softmax extends to multi-class with one-hot cross-entropy.
  \end{itemize}
\end{frame}
